# Stateful apps

In order to deploy stateful app, Kubernetes provides StatefulSet, which allows mounting PersistentVolumes using PersistentVolumeClaims.

But since I've chosen the difficult path with my own cluster, I needed to provide StorageClass and some provisioner.

I stumbled upon NFS using [nfs-subdir-external-provisioner](https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner). It provides a storage class and all underlying machinery needed for it.

I bought one more 50GB block volume for my VM, attached it to master node and installed `nfs-kernel-server`. After modifying `/etc/exports` to allow NFS access to a new block device and restarting the NFS daemon, I've got a ready-to-go storage server.

Next step is configuring the chart linked above. I changed NFS server hostname and mount path, plus tweaked policies to retain volumes instead of deleting them completely after pod is terminated, because along with TimeServer app I have deployed apps with important data (my knowledge base).

The new provisioner is named "nfs-reliable", as it is hosted on a node that never goes down and thus critical services may rely on it. 

After the cluster is ready to spawn new volumes, it's time to proceed with a lab.

## StatefulSet

I changed Deployment to StatefulSet and added one more volume mount and persistent volume claim:
```yaml
  volumeClaimTemplates:
  - metadata:
      name: timeserver-volume
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: 32Mi
      storageClassName: "nfs-reliable"
```

Helm upgrade... And voila...

```shell
> kubectl get pods --all-namespaces -o wide | grep timeserver
default                timeserver-0                                       1/1     Running   0          71s   192.168.63.240   mwp-master   <none>           <none>
default                timeserver-1                                       1/1     Running   0          64s   192.168.63.241   mwp-master   <none>           <none>
default                timeserver-2                                       1/1     Running   0          57s   192.168.63.242   mwp-master   <none>           <none>
```

Actually, we use ReadWriteOnce, so each pod has its own volume.

Let's ensure it is true after spamming F5 a bunch of times in a browser:
```shell
> kubectl exec timeserver-0 -- cat /volume/visits
5
```
```shell
> kubectl exec timeserver-1 -- cat /volume/visits
11
```
```shell
> kubectl exec timeserver-2 -- cat /volume/visits
9
```
We can see that Ingress/Service load-balanced our app to three replicas of our app and each of them has different amount of visits.

## Why ordering guarantee is unnecessary in this app
In some systems, app instances rely on each other and may form a chain, for example. However, our app is mostly stateless (the only current state is visits counter, which does not affect app logic in any way). Additionally, we even randomly load-balance requests to our service, so the particular pod is completely unnecessary. So, we may work with them in parallel. It is important to keep mind, though, that during update in parallel case all pods will go down at the same time, and we will have some downtime. In this pet project this is unimportant.

Let's add parallel pod management: `podManagementPolicy: "Parallel"`, delete stateful set and upgrade the chart:

```shell
> kubectl get pods --all-namespaces | grep timeserver
default                timeserver-0                                       0/1     ContainerCreating   0          3s
default                timeserver-1                                       0/1     ContainerCreating   0          3s
default                timeserver-2                                       0/1     ContainerCreating   0          3s
```

The final state of the deployment:
```shell
> kubectl get po,sts,svc,pvc
NAME                                                   READY   STATUS    RESTARTS   AGE
pod/ingress-nginx-controller-4bz9j                     1/1     Running   1          15d
pod/nfs-subdir-external-provisioner-7ccd6744fc-mk7tg   1/1     Running   0          3h41m
pod/timeserver-0                                       1/1     Running   0          8m22s
pod/timeserver-1                                       1/1     Running   0          8m22s
pod/timeserver-2                                       1/1     Running   0          8m22s

NAME                          READY   AGE
statefulset.apps/timeserver   3/3     9m4s

NAME                                         TYPE           CLUSTER-IP      EXTERNAL-IP     PORT(S)                      AGE
service/ingress-nginx-controller             LoadBalancer   10.103.230.22   192.168.1.240   80:31064/TCP,443:30319/TCP   15d
service/ingress-nginx-controller-admission   ClusterIP      10.99.212.226   <none>          443/TCP                      15d
service/kubernetes                           ClusterIP      10.96.0.1       <none>          443/TCP                      28d
service/timeserver                           ClusterIP      None            <none>          80/TCP                       28m

NAME                                                   STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
persistentvolumeclaim/timeserver-volume-timeserver-0   Bound    pvc-6c7b61ef-4e8f-4bd6-b614-b0159a6ec169   32Mi       RWO            nfs-reliable   3h15m
persistentvolumeclaim/timeserver-volume-timeserver-1   Bound    pvc-c547c3bb-787c-4376-980a-1968655a36c3   32Mi       RWO            nfs-reliable   3h15m
persistentvolumeclaim/timeserver-volume-timeserver-2   Bound    pvc-0f52fd2c-9eaa-4cb2-bbdb-0380c49129db   32Mi       RWO            nfs-reliable   3h15m
```


## Bonus: Recreate vs RollingUpdate strategies

The main difference is that recreate strategy destroys all old pods first, then create all new pods, and rolling update strategy destroys/creates pods one-by-one. Recreate eliminates different version inconsistencies, but introduces downtime. RollingUpdate introduces version inconsistencies, but keeps the app running at all times.

It is important to understand app business logic to pick what is more important. In a system that works with money or critical data it is probably better to have some downtime, but the data of the app will not be corrupted. If the app is non-critical, it is better to provide good user experience (no downtime).
